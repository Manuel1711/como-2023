{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amplitude regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a network to predict the amplitude for the process $gg\\rightarrow\\gamma\\gamma g$.  So the amplitude depends on the 4-momentum of 5 particles: 2 incoming gluons, 2 outgoing photons, and one outgoing gluon.  \n",
    "\n",
    "The incoming gluons will have no transverse momentum, but their total momentum along the beam pipe is not necessarily zero.\n",
    "\n",
    "The network we will train is a simple fully connected dense network.  This means that the input and output can only be vectors of real numbers.  We want to input the kinematic information on the particles to the network, and train the network to output the corresponding amplitude.  So we need to think about how we input our kinematic 4-vectors to the network, this generally depends on the network architecture we are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outline / tasks:\n",
    " - Imports \\& plotting set-up\n",
    " - Loading the data\n",
    " - Visualising the data\n",
    "     - visualise some of the kinematics of the process (transverse momentum of photons/gluons, MET)\n",
    "     - histogram the amplitudes\n",
    " - Preprocessing the data\n",
    "     - neural networks like $\\mathcal{O}(1)$ numbers\n",
    "     - how should we preprocess the data?\n",
    " - Datasets and dataloaders\n",
    "     - details are in the tensorflow docs\n",
    " - Building the neural network\n",
    "     - construct a simple neural network with 2 hidden layers with some number of nodes, it's up to you\n",
    "     - tip: use linear layers followed by relu activations\n",
    "     - all the relevant details are in the tensorflow docs\n",
    " - Plot the train and validation losses as a function of the epochs\n",
    "     - to check the the network is training we should plot the train and validation losses as a function of the epochs\n",
    " - Study the results\n",
    "     - beyond the losses, how can we check that our network is making sensible predictions?\n",
    " - Can you think of ways to improve your network or your training?  List them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.colors as mcolors\n",
    "import colorsys\n",
    "\n",
    "labelfont = FontProperties()\n",
    "labelfont.set_family('serif')\n",
    "labelfont.set_name('Times New Roman')\n",
    "labelfont.set_size(14)\n",
    "\n",
    "axislabelfont = FontProperties()\n",
    "axislabelfont.set_family('serif')\n",
    "axislabelfont.set_name('Times New Roman')\n",
    "axislabelfont.set_size(22)\n",
    "\n",
    "tickfont = FontProperties()\n",
    "tickfont.set_family('serif')\n",
    "tickfont.set_name('Times New Roman')\n",
    "tickfont.set_size(16)\n",
    "\n",
    "axisfontsize = 16\n",
    "labelfontsize = 16\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"mathtext.default\"] = \"rm\"\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dat = np.load( \"tutorial-2-data/trn_dat.npy\" )\n",
    "trn_amp = np.load( \"tutorial-2-data/trn_amp.npy\" )\n",
    "\n",
    "val_dat = np.load( \"tutorial-2-data/val_dat.npy\" )\n",
    "val_amp = np.load( \"tutorial-2-data/val_amp.npy\" )\n",
    "\n",
    "tst_dat = np.load( \"tutorial-2-data/tst_dat.npy\" )\n",
    "tst_amp = np.load( \"tutorial-2-data/tst_amp.npy\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( f\"train data shape: {trn_dat.shape}\" )\n",
    "print( f\"train amp  shape: {trn_amp.shape}\" )\n",
    "print( f\"test  data shape: {tst_dat.shape}\" )\n",
    "print( f\"test  amp  shape: {tst_amp.shape}\" )\n",
    "print( f\"val   data shape: {val_dat.shape}\" )\n",
    "print( f\"val   amp  shape: {val_amp.shape}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is organised as follows:\n",
    " - the shape corresponds to ( number of events, number of particles, 4-momentum )\n",
    " - the particles are arranged as follows\n",
    "     - the first two entries are the two incoming gluons\n",
    "     - the next two particles are the outgoing photons\n",
    "     - the last particle is the outgoing gluon\n",
    " - the incoming gluons and incoming photons are arranged by transverse memontum $p_T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will make some kinematic plots of the events in the training sample.  Note however that these are not the physical distributions we would measure at the LHC!  In our training data each of these events is associated with an amplitude, which tells us the probability that the event will be produced in the gluon-gluon interaction.  So to get the physical distributions these events would need to be 'weighted' by their amplitude.  However, right now we just want to visualise our training dataset to see what preprocessing we should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_pz( ev ):\n",
    "    return ev[0][3] + ev[1][3]\n",
    "\n",
    "def get_mass( fv ):\n",
    "    msq = np.round( fv[0]**2 - fv[1]**2 - fv[2]**2 - fv[3]**2 , 5 )\n",
    "    if msq>0:\n",
    "        return np.sqrt( msq )\n",
    "    elif msq<0:\n",
    "        raise Exception( \"mass squared is less than zero\" ) \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_pt( fv ):\n",
    "    ptsq = np.round( fv[1]**2 + fv[2]**2 , 5 )\n",
    "    if ptsq>0:\n",
    "        return np.sqrt( ptsq )\n",
    "    elif ptsq<0:\n",
    "        raise Exception( \"$p_T$ squared is less than zero\" ) \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_met( ev ):\n",
    "    return np.abs( np.sum( [ fv[1]+fv[2] for fv in ev ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the initial $p_z$ of the events in the training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#....#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot a histogram of the amplitudes for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amplitudes span about 4 orders of magnitude..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the leading photon $p_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the final state gluon $p_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean final state gluon $p_T$ in GeV is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the missing transverse energy (MET) for each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a dense network, so the data needs to be in vector format.  We will collapse the data for each event to a single vector of dimension $5\\times4=20$.  The fact that the data is ordered here is important.  To predict the amplitude given the kinematics, the network needs to know which entries correspond to which particles in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nev = trn_dat.shape[0]\n",
    "trn_datf = np.reshape( trn_dat, (nev,-1) )\n",
    "val_datf = np.reshape( val_dat, (nev,-1) )\n",
    "tst_datf = np.reshape( tst_dat, (nev,-1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_datf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are further preprocessing steps we can take.  For example, the inputs are numerically very large $\\mathcal{O}(100)$ and span a large range.  So we could re-scale the inputs by a constant number, or even take the logarithm of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_datf = #....#\n",
    "val_datf = #....#\n",
    "tst_datf = #....#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also preprocess the amplitude data.  As we seen in a plot above, the amplitudes span about 4 orders of magnitude.  This could be difficult for the network to interpolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ampl = #....#\n",
    "val_ampl = #....#\n",
    "tst_ampl = #....#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new distribution looks nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#....#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a simple network with one input and one output layer, and two hidden layers.  We define the dimensions of these layers below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipt_dim = #...#\n",
    "opt_dim = 1\n",
    "#..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(ipt_dim, opt_dim, #....#):\n",
    "    input_la = keras.Input(shape=(ipt_dim,)) #input layer\n",
    "    #....#\n",
    "    model.add(layers.Dense(opt_dim)) #output layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to decide what function we want the neural network to optimise, i.e. the loss function.  There are a number of choices to decide from, the key point is that the loss function should be minimised when the neural network correctly predicts the amplitude given the kinematical information on the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(ipt_dim, opt_dim, #....#)\n",
    "loss_mse =  #loss function\n",
    "opt_SGD =  #optimizer\n",
    "model.compile(loss=loss_mse, optimizer = opt_SGD)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(#....#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the train and validation losses as a function of the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.....#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't used the test data here.  The test data becomes useful if our neural network overfits the training data, and we want to stop training the network early.  In this case we use the validation data to decide at which epoch to stop training, based on some early stopping condition.  For example, if the validation loss does not improve for 10 epochs, we might decide to stop training.  If we have saved the network at certain epochs during training, we would then select the network with the smallest validation loss to use.  The test data is then the final dataset we test on, since it was not used during training or in the early stopping procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to get some visualisation of how well our amplitude regression has worked.\n",
    "\n",
    "The simplest thing we can do is to pass our data through the neural network to get a predicted amplitude for each event, then histogram this and compare it to the histogram of the true amplitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_trn_ampls = #....#\n",
    "pred_val_ampls = #....#\n",
    "pred_test_ampls = #....#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#....#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams[\"text.usetex\"]  = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to visualise the error on the predicted amplitudes by plotting $\\text{abs}\\left((A_{\\text{true}}-A_{\\text{pred}})/A_{\\text{true}} \\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then for the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#....#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#....#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
